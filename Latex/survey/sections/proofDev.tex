\section{Methodologies for Efficient and Maintainable Proof Development}
\label{sec:proofDev}


%Automation
%Organization (modularity, framework, compositionality )
%Abstraction
%Reuse (ornaments, combinators) 

%Formalization in large-scale demands for methodologies to make proof development process efficient or even feasible for complex systems. For a long time, mere proof creation rather than practical proof engineering for maintainability had been the primary concern of the Proof assistant designers~\cite{Curzon_1995}. However, recent advancement in large-scale verification has changed their perception. Large complex systems reap the benefit of formal verification the most, which in turn creates gigantic and complicated proof scripts. Also, proof development itself is an iterative process ~\cite{Woos_et_al_2016} and sometimes gets carried out as part of an active, hence evolving development project. Thus, f

Recent advancement in large-scale verification has expanded the focus of interactive theorem proving research from mere proof creation to practical proof engineering aspects~\cite{Curzon_1995}. Researchers are trying to come up with methodologies for designing well-organized and adaptable proofs to make development process efficient and maintainable~\cite{Kaivola_et_al_2003, Woos_et_al_2016, Curzon_1995}. The reusability of proofs also plays an essential role in promoting efficiency and maintainability. In the following two subsections, methodologies accumulated across literature are discussed under the two subcategories - a) methodologies for adaptability and organization, b) methodologies for reusability along with an account for how these strategies differ concerning ITP systems when applicable.

Moreover, proof engineers also benefit from some form of automation within the ITP as well as from the portability of proofs among different ITP systems to improve their performance, thereby efficiency of the overall proof development. A third subsection elaborates on gains from automation and portability as well as existing methods to facilitate them.

\subsection{Methodologies for Proof Organization and Adaptability}

%reason this way as adaptability facilitate efficiency
%Adaptability and maintainability go hand in hand. Proofs, to be well maintained, needs to be adaptable to changes. Unfortunately, interactively created proofs, although through hard-labor, can easily get broken in the face of changes and requires a substantial amount of time to recover, unless good design strategies are followed during proof creation. This brittle nature of proofs, together with the amount of required recovery work, has been identified as one of the major causes of inefficiency in many large proof developments (see section~\ref{sec:ITPinLarge} for references). 

Crafting robust, easily modifiable, strategically structured proofs play an important role in increasing efficiency during development. Features like modularity and abstractions in ITP systems, as well as custom or general-purpose tactics, can promote well organization and adaptability in proof development. \\

\textbf{Through Modularity and Abstraction} Modularity and abstraction can be used to write structured proofs. Organizing proof with good structuring principle often helps developers to compartmentalize their thoughts as well. For example, decomposing complex problems into simpler ones hierarchically based on their mutual dependencies lets the developer focus on one problem at a time. It also makes proving higher-level problems easier with already proved simpler ones they depend on. Software engineering has several methodologies for organizing and writing adaptable codes. Although these methods can give ideas for proof engineering, they often need to be improvised to be used in proof development. Information hiding from dependents or clients is one of these software development techniques, albeit a classic one. The trick here is one can not depend on the information they do not know, thus forcing dependents to write implementation without that information. This technique lets code to be updated without any changes to its clients. Proof engineering can also use this same technique by hiding functions' and types' definitions behind interfaces and reduce unnecessary rework. However, in proof development, dependents do not just use these definitions; they need to reason about them, which often demands more information. For instance, systems based on intensional type theory like CoQ depend on \emph{definitional equality}, thus needs detailed constructional information of function and types as well. Planning for change by Woos \emph{et. al.}~\cite{Woos_et_al_2016} addresses this problem by adding lemmas in the interface that are sufficient and necessary to reason about themselves while hiding the rest of the information. To be noted here, systems based on extensional type theory, like Nuprl, would not need these additional interface lemmas when reasoning for equality (see section~\ref{sec:overview} for detail). They also recommend using the interface to separate theorem statements from their proofs. This technique also leads to faster development. In proof assistants like Coq, checking after modifying one proof kicks off rechecking of all other proofs that depend on it. Thus, the developer needs to wait a while after each edit, and in a large development, 'a while' gets really long. Following the above recommendation, dependent proofs would only have to import interfaces containing statements. Unless interfaces are not modified, the rechecking of other proofs is not needed anymore. Authors have reported the attainment of 100X faster build time by following this rule. 

However, the realization of any of the above methods requires interfaces. Interfaces can be implemented through modules in proof assistants. CoQ module system, inspired by modules in standard ML, provides support to accumulate abstract types with operations over them~\cite{Chlipala_CPDT_2013}. Functors, another feature borrowed from ML, which are functions from modules to modules, are also supported in Coq that let create parametric modules. Coq also provides both options of either hiding or revealing implementation details when ascribing signatures. Chlipala in~\cite{Chlipala_2008} has taken the advantage of the Coq module system to build a generic program verification tool with a library of reusable functors. This tool can be used to prototype verifiers for various type systems along with soundness certificate. Other example implementations of the module system in Coq are Finite sets and Maps using AVL trees~\cite {Filliatre_et_al_2004}, balanced binary search trees using red-black trees~\cite{Appel_2011}. Agda also offers a module system. However, Agda modules can contain submodules along with definitions, thus allowing hierarchical organization within it. Functions defined in a module can be declared private to make them inaccessible from outside. Parameterization can be done through abstracting several arguments from multiple functions inside the module at once and making these arguments, parameters of the module instead. It is different from the use of functor in CoQ modules but analogous to sections in CoQ. Isabelle provides module system through extension called locales~\cite{Ballarin_2004, Kammuller_1999} that supports general parameterization~\cite{Ballarin_2010}. Rabe and Sch\"{u}rmann also, through a conservative extension, propose a module system for the logical framework for LF~\cite{Rabe_et_al_2009}. It has been implemented as part of the Twelf distribution and used to modularize big part of Twelf example library without any loss in efficiency. Type classes and canonical structures can also provide means for abstraction, and a detailed overview can be found in~\cite{Ringer_et_al_2019}. 

Even with all the module support from the system, structuring and decomposing code to achieve effective modular reasoning still require additional engineering and is a challenging open problem. Although lots of factors from the underlying system dictate the ultimate proof structure, there are some general principles that can be followed irrespective of that. One such general methodology can be found in Kaivola and Kohatsu's industrial-size circuit verification work~\cite{Kaivola_et_al_2003}. Here, they structure their proof scripts by first layering the definitions hierarchically. Each definition layer captures a single aspect of the underlying system with associated claims. The higher-level definition can use lower levels' claims to reason about them. Then, they packaged all proofs that require internal details of a definition with the definition itself. Although there can be a proliferation of claims if definitions are refined too much, it still is a good technique to keep in mind when structuring proof. However, their working environment did not support modules, and they had to emulate them through conditional load sequences which incurred additional work. Using the custom induction principle is another way to achieve modularity~\cite{Woos_et_al_2016}. Induction is one of the widely used methods of theorem proving in proof assistants with inductive data types. In these cases, common inductive arguments can be factored out into custom induction principles. Then, they can just be proved once and used throughout the system. Proofs that use induction principles remain valid after changes that maintain induction patterns and might just require reproving of the induction principles. 

Refinement proof can also help in the organization. Abstract specification and low-level executable specification of a system can be related through refinement proofs in a way that if anything is true for the abstract specification, then it is true for implementation as well. Therefore, both specification and implementation can be kept and maintained separately. This can make low level changes like adding simple features less costly~\cite{Klein_et_al_2014}. \\ 

\textbf{Through Tactics}
In tactic based theorems, use of general purposed tactics promote adaptability by making proof scripts robust to changes. These tactics can dispatch similar but different goals and proof scripts stay valid as long as modified goals can be handled by them. For example, \emph{lia}, the newer version of the popular \emph{omega} tactic, in CoQ can solve quantifier-free problems in Presburger Arithmetic. As a result, goals can be modified without breaking proof as long as they remain within the realm of \emph{lia}'s solving capability. Custom tactics can also be written to facilitate robustness in tactic proofs. One of the recommendations in Planning For Change~\cite{Woos_et_al_2016} for robust development, is not to depend on automatically generated hypothesis names or hypothesis ordering by systems. Any changes in the original goal is likely to modify these names and orders causing proofs to break if not followed the recommendation. One way to avoid this is to assign explicit names to hypotheses and the IDE Company-Coq has built-in support for that. However, these names also require maintenance in changing context. Instead, writing custom tactics, using tactic language like Ltac, to specify declaratively which hypothesis to be used rather than using names solve the problem more elegantly. For instance, the tactic \emph{find\_rewrite}, used by the authors in~\cite{Woos_et_al_2016}, can automatically find equality in the context and rewrites everywhere by it. StructTract library~\cite{StructTact_2016} has more custom tactics like this such as \emph{break\_match} and \emph{prep\_induction}. 

\subsection{Methodologies for Proof Reusability}

Proof reuse can maximize the efficiency of developers by letting them use existing proved theorems and lemmas as much as possible, thereby reducing works when creating or adapting proofs. Several methods have been discussed across the literature to enable proof reuse. On a higher level, all these techniques are basically based on any one of the following three - a) proof term transformation or b) proof script generalization or c)exploiting type equivalence relationship. \\ %Below, I'll describe some techniques from a subset of them and try to cite all the other related research. 

\textbf{Through Proof Term Transformation} 
Proofs can be generalized by transforming associated proof terms in an ITP system whose underlying type theory represents theorems as types and proofs as terms of these types following Curry-Howard isomorphism. Examples of such ITP systems include Coq, Nuprl, Lego. Often times, proof development ends up with sets of specific theorems that are just specialized versions of general theorems. Hence, once a proof is proved for a specific theorem, one can generalize it to facilitate its later reuse. Related experimental work can be found in the Ph.D. thesis of Olivier Pons~\cite{Pons_1999}, which is, in fact, one of the earliest works on generalization through proof term transformation. The main idea in this thesis is to get an abstract version of the proof term by exploiting the dependency relationship of objects in the proof and then, use this in other places with appropriate instantiation. Another source of repetitive work in proof development is when the inductive types are extended with new constructors or new parameters. Developers again need to prove or adapt all the lemmas and theorems associated with the modified type. However, theorems on inductive types are usually proved by case analysis or by induction. Most cases of existing proofs can be reused without modification, and just the new cases require the developer's attention. Boite in his paper~\cite{Boite_2004} proposed a tool that adapts existing proofs to these modifications by transforming associated proof terms. However, cases for new constructors are just automatically added in the adapted proof and developer needs to complete these cases by themselves. Mulhern's Proof Weaving~\cite{Mulhern_2006} paper takes this process one step further. They proposed a method that attempts to automatically synthesize proofs for these new cases from the repetitive pattern of the existing proof structure. Consequently, the developer only needs to deal with non-repetitive interesting cases. LCF based proof assistants, by default, do not create an explicit proof term and provide soundness by construction. However, to be able to use techniques that rely on proof term transformation, Berghofer, and Nipkow~\cite{Berghofer_Nipkow_2002} proposed proof term construction method for LCF and implemented it in the proof assistant Isabelle. Manipulation of these proof terms in Isabelle is also less involved than it is in proof assistants with richer type theory e.g., CoQ. This allows for more complex transformation such as both function and types abstraction without imposing any restriction on proofs themselves~\cite{Boite_2004}. 

However, Caplan and Harandi~\cite{Caplan_Harandi_1995} used a different approach in their work on the verification of reusable software components. They provide a logical framework by embedding a quantified version of Hoare logic into typed lambda calculus. This framework allows the simultaneous construction of abstract programs and its accompanying abstract proofs thus, enabling reuse of both through specialization. \\

\textbf{Through Proof Script and Tactic Generalization} Reusability can also be attained through abstraction at the level of proof scripts or tactics. Generalizing tactic based proofs allows reusing part of existing proofs after any changes to goal or specification. Felty and Howe~\cite{Felty_Howe_1994} proposed an ITP system that generalizes its tactic proofs with metavariables after each tactic is applied. When a change is made, the reuse operation of this system solves a higher-order unification problem to find part of the existing proof that can be reused. Then it presents the adapted proof with remaining subgoals to the users. Their system is almost like Nuprl but with less restriction on inference justification to enable the use of metavariable that is not supported in Nuprl. 

Moreover, some proof assistants provide features like modules and higher-order parameterization. These features facilitate separation of concerns and parameterization of proof specification, which could be used to promote reusability and maintainability in proof scripts. Delaware \emph{et. al.}~\cite{Delaware_et_al_2011} exploited these in their work on verifying product lines of programming language. Product lines programs are decomposed into features, and variants of products are synthesized from composing different combinations of them. For composition, each feature uses variation points to capture the interaction between them. For example, the extension point of a data type construct in one feature when composing with another is marked by a variation point. On the verification side, authors also feature decomposed theorems into modules. Then, inside each feature module, variation points are encoded in theorems by higher-order parameterization. These modules now can be certified independently by appropriate assumptions on the variation points, thereby not requiring any recheck of existing proofs after compositions. Other formalization with similar needs like abstraction and modularity can use this technique in any higher-order proof assistant. \\

\textbf{Through Type Equivalence} In recent years, exploiting type equivalence relationships to facilitate reusability is seeing increasing popularity among proof engineering researchers. These relationships promote reuse by allowing functions and proofs, written for one type, to be carried over to an equivalent type. Earlier related works include Barthe and Pons' work on Type Isomorphism in dependent type theory~\cite{Barthe_Pons_2001}. They proposed an extension to dependent type theory by enforcing type isomorphism through rewrite rules at the level of types and elements. These computation rules let proof developers go back and forth between different representations of the same structure or object and reuse components over them. Similarly, a relationship can be established between the refined version of a datatype to its respective unrefined one to support reusability. Ornaments, proposed by McBride in~\cite{McBride_2011}, can be used as a notion on a data type to define that relationship. These ornaments allow derivation of fancy data types, both indexed or annotated with additional data, from the plain ones. For example, vectors can be expressed as ornamented lists just as well lists can be expressed as ornamented numbers. Functions and proofs can be ported from one type to another if they are related through ornaments~\cite{Dagand_McBride_2012}. Ornaments are supported in Agda as deep embedding~\cite{Williams_2014} and used in a proof reuse tool, DEVOID in coq~\cite{ringer_et_al_2019_b}. However, it takes linear time to derive conversions between types through an ornamental relationship where Diehl \emph{et. al.}~\cite{Diehl_et_al_2018} provides a constant time conversion between indexed and non-indexed types and functions over them in both directions. They use an extrinsic type theory, CDLE, to achieve this zero performance penalty, although their work is restricted to only index refinement aspect in contrast to the ornament, which also supports annotation. The same idea of proof reuse can also be applied with Voevodsky's \emph{univalence axiom} in Homotopy Type Theory(HoTT) which provides the correct notion of \emph{equality} to be used in theorem prover. \emph{Univalence axiom} states type identity in HoTT to be equivalent to type equivalence~\cite{Awodey_et_al_2013}. Cubical Type Theory, by Cohen \emph{et. al.}~\cite{cohen_et_al_2018} provides constructive justification for this \emph{univalence axiom}. Therefore, extensionality principles like \emph{functional extensionality} and \emph{propositional extensionality} can be directly proved in theory. Cubical Type Theory was further extended by Coquand \emph{et. al.}~\cite{Coquand_et_al_2018} for higher inductive types which made it possible to integrate it in Agda as Cubical Agda. Nuprl has also been extened to a proof assistant, RedRpl based on cubical computational type theory~\cite{Angiuli_et_al_2018}. Coq also followed the same path with their CoqHoTT project, which stands for Coq for HoTT~\cite{CoqHoTT_2015}. Tabareau \emph{et. al.}~\cite{Tabareau_et_al_2018} have used HoTT, though without univalence, to provide a framework for theorem and definition transfer between equivalent types in Coq. In a nutshell, it can be said that researchers are finding new ways for entities to be equal that can be exploited to promote reuse, and recent developments in HoTT have the potential to make a breakthrough in this area of research~\cite{Ringer_et_al_2019}. 

%Finding these type equivalence and integrating them in ITP systems to promote reuse in proof development is challenging work. 
%Both of this works made it possible to integrate HoTT and Cubical Type Theory into several ITP systems e.g., RedRpL extending Nuprl~\cite{}, Cubical Agda as an extension for Agda, CoqHoTT as an extension for Coq. 
%https://homotopytypetheory.org/2015/12/02/the-proof-assistant-lean/

%\subsection{Strategies for Efficient Coding and Manageable Code Base}

%automation tactic proof lang to faster dev
% - paper theorem tactice reuse gives a new interactive autom strat
%declarative language
%code reviewability
%modules + proof int he large proof structure


%In a way, these characteristics also facilitate maintainability over time thereby can be seen as proactive features to handle proof evolution.

\subsection{Methods for Automation and Portability}
There are several other methods and technologies that can substantially improve the efficiency of the developers beyond those that fall in the above categories. Among them, automation and portability among ITP systems are worth surveying in the scope of this work. 

\subsubsection{Automation}
Automation combined with human-guided theorem proving can amplify human's ability to a great extent. In the earliest era of machine-assisted verification, people were mostly interested in truly automated theorem proving(ATP). However, realizing its limitation and at the same time, the power of human interaction has shifted the direction away from the automation to the more interactive arrangement. Harisson \emph{et. al.}~\cite{Harrison_et_al_2014} has an interesting historical depiction on this. Nowadays, automation plays roles of moving burden away from the programmer by taking charge of repetitive, trivial tasks and reutilizing large base of previous works. Two useful and well-developed automation techniques called Theory exploration and Hammers are briefly discussed here to illustrate how they can improve the efficiency of interactive theorem proving. \\

\textbf{Theory exploration} Theory exploration is a powerful technique from the automated theorem proving (ATP) era for automatic discovery of lemmas during a proof development. There is no reason not to use the proven techniques from long research in ATP within ITP. One such combination has been used by Dramnesc \emph{et. al.} in their work of construction of a theory of binary trees~\cite{Dramnesc_et_al_2015}. They perform their systematic exploration of theories in an automated theory exploration tool called \emph{Theorema}~\cite{Buchberger_et_al_2006} but prove properties for the synthesized algorithm in interactive theorem proving tool Coq. Hipster~\cite{Johansson_et_al_2014} is another successful initiative in this area which integrates theory exploration with Isabelle/HOL. It has two modes of operations - \emph{explotary mode} that generates lemmas from a set data types and functions and \emph{proof mode} that tries to discover missing lemmas during a proof creation for the current goal to be proved. \\

% Hipster: https://arxiv.org/abs/1405.3426
% Theorema: Buchberger et al. 2006  
% Theorema+co: Dramnesc et al. (2015) 

\textbf{Hammers} Hammers is the technique that truly exploits ATP in interactive arrangements. The target is to automatically build a proof to discharge a goal in a proof assistant. Typically, the process is decomposed into three components. Premise selector explores large formal libraries associated with proof assistants to find premises related to the conjecture. Methods like Naive Bayes and K-nearest neighbors can be used in this step for mining the libraries~\cite{Blanchette_et_al_2016}. This component independently can be functional for users to find useful lemmas, especially when libraries are huge like Mizar Mathematical Library and trained machine learning tools like Mizar Proof Advisor~\cite{Urban_2004}. The second component, Translation module, converts these selected premises and the current goal from proof assistant's logic to target ATP's logic, thus creating an ATP problem for target ATP to solve. If a proof is found by the target ATP, the final component, proof reconstructor, reduces the ATP proof to inferences of proof assistant's kernel to make the proof certifiable by the proof assistant. Several Hammers tools for different ITPS are already available including CoqHammer~\cite{Czajka_Kaliszyk_2018} for Coq, Sledgehammer~\cite{Paulson_et_al_2015} for Isabelle/HOL and HOL$^Y$Hammer~\cite{Kaliszyk_et_al_2014} that is compatible with both HOL Light and HOL4 and, comes with an online extension as well~\cite{Kaliszyk_et_al_2015}.  

%\textit{Hints databases}It is also possible to make search carried out by tactics like auto or rewrite more robust by giving specific databases to search within. Several hint databases are already there in the Coq standard library like core, Arith. User can create their own databases. 

%Optimal balance of automation and interaction is a  

%However, interactive theorem is not just about having machine to check human written proof merely. 
%There are three styles of automation in practice - through tactics, through proof languages and through proof by reflection. Agda only supports proof by reflection. Coq and Isabelle have support for all three of them. 

\subsubsection{Portability among ITP Systems}
%normal and also code extraction
Having a mechanism to import and export proofs between different ITP systems i.e., portability of proofs, has multiple benefits in terms of increasing development efficiency. So far, we have discussed several dominant proof assistants that have been built as stand-alone programs with fundamental differences among them. Although distinct and unique features from different systems are advantageous for supporting a variety of work and personal preferences, they effectively fragmented the work base. Large libraries and huge-scale verification work from one ITP system community are now merely useful to others, and reproducing them requires a substantial amount of time. Therefore, transferring work from one domain ITP to another through portability can collectively increase the efficiency of proof engineers and facilitate collaboration and idea transfer among them. 

However, proof translation from one system to another is not straightforward due to their difference in underlying theoretical foundations. The target system requires having at least richer logic than the source system so that it can accommodate all constructs in proofs written in the source. Moreover, translation of proof's statements from system to another is not the only task at hand here, though that can be classified as one form of translation. For complete portability, users of the target system should be able to recheck translated proofs at their own kernel instead of relying on the correctness of translator and source kernel. Numerous proof translators between different ITP systems can be found in literature. For example, Naumov \emph{et. al.}~\cite{Naumov_et_al_2001} describes a proof translator from HOL to classical extension of Nuprl. This work is based on a series of earlier works on HOL to Nuprl translation~\cite{Howe_1996_a, Howe_1996_b, Felty_1997} that provides a good insight on how theoretical foundation is built brick by brick before getting to the final translator. One step in this trail of background works was to represent HOL as a submodel of the richer Nulprl model~\cite{Howe_1996_a} to meet the specification of having a richer target system. However, McLaughlin's work in~\cite{McLaughlin_2006} shows that having at least richer target system is not an absolute requirement for translation. In this paper, authors successfully have translated a part of Isabelle/HOL (source with richer logic) standard library into HOL light (target) with a clever workaround. The part of Isabelle/HOL logic that cannot be represented directly in HOL Light object logic is instead elaborated in HOL Light's metalanguage, OCaml using functors. Other notable works in proof translations are hol90 into CoQ~\cite{Denney_2000}, HOL4 and HOL Light into Isabelle/HOL~\cite{Obua_2006, Kaliszyk_Krauss_2013}, HOL Light into Coq~\cite{Keller_Werner_2010}. There is also a framework, called OpenTheory, that can transport proofs between HOL family~\cite{Hurd_2011}. A recent study on the interoperability of proof assistants as well as the integration of their libraries by translating them into a universal format, highlights the challenges in this field more broadly~\cite{Kohlhase_Rabe_2016, Kohlhase_Rabe_2020} as well as takes the famous QED dream, a computer-based database of all mathematical knowledge formalized and machine-checked~\cite{QED_Manifesto_1994}, one step ahead. 

%below1:https://kwarc.info/people/frabe/Research/KR_oafexp_20.pdf

%, almost like reasoning with interfaces lemmas are used to reason about in information hiding technique~\cite{Woos_et_al_2016}
% how to structure code to get modular However, even in a system without support for modules, modular reasoning can be attained by emulating them through conditional load sequences like verification project ~\cite{Kaivola et al. }, they provide a framework for modular development without any explicit module support from their work environment. To emulate modules, authors first layer their definition hierarchically, capturing different aspects of the system in a single layer and then packaged all proofs that require details of a definition with the definition itself. Nevertheless, choosing a different system  Type classes and canonical structures can also provide means for abstraction, and a detailed overview can be found in ~\cite{Ringer itself}. 


